{\rtf1\ansi\ansicpg1252\cocoartf1561\cocoasubrtf600
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fswiss\fcharset0 ArialMT;}
{\colortbl;\red255\green255\blue255;\red16\green40\blue66;}
{\*\expandedcolortbl;;\cssrgb\c7059\c21176\c32941;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid1\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}}
\paperw11900\paperh16840\margl1440\margr1440\vieww25400\viewh13680\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 Machine learning course work code running.\
\
1. Our initial descriptive statistics is done on the data set having 21 engineering features which are derived from converting the initial 6 high level features + all respective sub-attributes into Individual car features columns.\
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl300\partightenfactor0
\ls1\ilvl0
\f1\fs26\fsmilli13333 \cf2 {\listtext	\uc0\u9642 	}\expnd0\expndtw0\kerning0
1. buying: vhigh, high, med, low.\'a0\
\ls1\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u9642 	}\expnd0\expndtw0\kerning0
2. maint: vhigh, high, med, low.\'a0\
\ls1\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u9642 	}\expnd0\expndtw0\kerning0
3. doors: 2, 3, 4, 5more.\'a0\
\ls1\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u9642 	}\expnd0\expndtw0\kerning0
4. persons: 2, 4, more.\'a0\
\ls1\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u9642 	}\expnd0\expndtw0\kerning0
5. lug_boot: small, med, big.\'a0\
\ls1\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u9642 	}\expnd0\expndtw0\kerning0
6. safety: low, med, high.\'a0\
\pard\pardeftab720\sl300\partightenfactor0
\cf2 \
our bar plot shows the counts for each of the 21 features. You will notice from the bar plot that features with 4 sub attributes have lower counts and those with 3 sub attributes have higher counts. This is because the data set is well-curated and available on the UCI site. Of greater interest than these counts, is perhaps gaining some knowledge with respect to feature importance, we initially did this using fitcensemble as a learner is required in order to abstract an importance metric for features. We feel this worthwhile showing in order to provide the reader with an initial understanding of important features. The findings, clearly illustrate importance metrics which seem reasonable and in-line with desirable car features.
\f0\fs24 \cf0 \kerning1\expnd0\expndtw0 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 \
2. Our main pre-processing files is Train_test_split_21.m which calls a number of other functions for encoding of categorical features. Following this complete 80/20 random train-test splitting but save the workspace to the .mat file \'91Train_test_21.mat\'92.  This same .mat file is loaded by both of our models to ensure exact same train and test data is presented to either one.\
\
3. We complete a bootstrap aggregation and extend that to a random forest for the 21 variables inside RF_train_test_21.m (standard and tuned models are both implemented within this file). We also complete the same exercise for the Naive babes model. In this case the tuned model is in a separate m-file from the standard model.\
\
4. As an aside, We also looked at completing the same models with just the initial 6 categorical variables converted to numeric but not expanded to 21 features. Our main focus was using the 21 feature variables as this enabled us to compare output with other completed studies.\
\
}